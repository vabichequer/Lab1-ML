{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees - Iris dataset\n",
    "\n",
    "Now we will use decision trees to address one of the most known database in machine learning. It was proposed by R.A. Fisher in 1936 and the goal is to classify instances of iris plants in three subspecies: iris-virginica, iris-setosa and iris-versicolor. The attributes describing each instance are the sizes (length and width) of the petal and sepal.\n",
    "The dataset contains 150 examples (each of the 3 classes is represented by exactly 50 instances). \n",
    "\n",
    "<img src=\"iris.jpg\">\n",
    "\n",
    "In order to implement the models we will use the class <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\">\n",
    "DecisionTreeClassifier</a> from *sklearn.tree*. \n",
    "\n",
    "First we import the libraries we will need. In addition we will use the first code cell to activate the *inline* mode for the graphics generated by *matplotlib*. We also initialize the seed of the random generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib as matl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the Iris problem dataset. This problem is so famous that the dataset is included in the module *sklearn.datasets*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to visualize the data. As we can observe the problem is not too hard. One of the classes (setosa) is completely separated from the other two, which are slighly overlapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,15))\n",
    "n_classes = 3\n",
    "plot_colors = \"bry\"\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\n",
    "    X = iris.data[:, pair]\n",
    "    y = iris.target\n",
    "\n",
    "    plt.subplot(3, 2, pairidx + 1)\n",
    "    plt.xlabel(iris.feature_names[pair[0]])\n",
    "    plt.ylabel(iris.feature_names[pair[1]])\n",
    "    plt.grid(True)\n",
    "        \n",
    "    plt.plot(X[y==0,0], X[y==0,1], 'bo', label=iris.target_names[0][0:3])\n",
    "    plt.plot(X[y==1,0], X[y==1,1], 'ro', label=iris.target_names[1][0:3])\n",
    "    plt.plot(X[y==2,0], X[y==2,1], 'yo', label=iris.target_names[2][0:3])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\">\n",
    "DecisionTreeClassifier</a>. The most important arguments for DecisionTreeClassifier builder are the following: \n",
    "\n",
    "- *criterion:* criterion for splitting the tree nodes. It can be 'gini' or 'entropy' (this last is equivalent to information gain).\n",
    "\n",
    "- *max_depth:* maximum depth of the decision tree.\n",
    "\n",
    "The examples reaching a tree node are used to compute statistics related\n",
    "to estimate the quality of subsequent splittings at that node. The examples\n",
    "are also used to compute statistics related to the class to be predicted\n",
    "in case no further splittings are made. This number should be large enough\n",
    "to ensure these statistics quality.\n",
    "Thus requirements about the minimum amount of examples are needed:\n",
    "\n",
    "- *min_samples_split:* minimum number of examples in a tree node required to be splitted.\n",
    "\n",
    "- *min_samples_leaf:* minimum number of examples in a classification node\n",
    "\n",
    "In next example we will use the Gini criterion and will let the tree grow not imposing any maximum depth.\n",
    "In order to visualize the results more easely we will only use *petal_length* and *petal_width* attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris.data[:,[2,3]] # Los atributos 2 y 3 son petal_length y petal_width\n",
    "y = iris.target\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "clf = clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell is used to visualize the constructed decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libreria_aux_arboles import tree_to_code, tree_to_pseudo\n",
    "\n",
    "startbold = '\\033[1m'\n",
    "endbold = '\\033[0m'\n",
    "\n",
    "tree_to_code(clf, [iris.feature_names[2], iris.feature_names[3]],\n",
    "             start_bold=startbold, end_bold=endbold)\n",
    "\n",
    "#from graphviz import Source\n",
    "#Source( tree.export_graphviz(clf, out_file=None,\n",
    "#                             feature_names=[iris.feature_names[2], iris.feature_names[3]],\n",
    "#                             class_names=iris.target_names,\n",
    "#                             filled=True, rounded=True,\n",
    "#                             special_characters=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we show the classifier's decision frontiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plot_step = 0.02\n",
    "x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "z = z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, z, cmap=plt.cm.Paired)\n",
    "\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "\n",
    "plt.plot(x[y==0,0], x[y==0,1], 'bo', label=iris.target_names[0])\n",
    "plt.plot(x[y==1,0], x[y==1,1], 'ro', label=iris.target_names[1])\n",
    "plt.plot(x[y==2,0], x[y==2,1], 'yo', label=iris.target_names[2])\n",
    "    \n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform the following improvements:\n",
    "\n",
    "- We collect all previous code in a few cells so it is easier to perform different experiments.\n",
    "- We include a training set/test set partition of the database in order to properly validate the model. This allows to measure the predictive quality of the model by means of the scoring and the confusion matrix.\n",
    "\n",
    "Try different parameters and respond to the questions made at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# training /test split\n",
    "testsize = 0.3 # in the [0,1] range. 1: 100%\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=testsize, random_state=5)\n",
    "\n",
    "# Decision tree construction ---------------------------------------------------------------------\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=2, min_samples_split=2, min_samples_leaf=1)\n",
    "clf = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_to_code(clf, [iris.feature_names[2], iris.feature_names[3]], start_bold=startbold, end_bold=endbold)\n",
    "\n",
    "\n",
    "# Tree visualization --------------------------------------------------------------------\n",
    "#Source( tree.export_graphviz(clf, out_file=None,\n",
    "#                             feature_names=[iris.feature_names[2], iris.feature_names[3]],\n",
    "#                             class_names=iris.target_names,\n",
    "#                             filled=True, rounded=True,\n",
    "#                             special_characters=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision frontier -----------------------------------------------------------------------\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plot_step = 0.02\n",
    "x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "z = z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, z, cmap=plt.cm.Paired)\n",
    "\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "\n",
    "plt.plot(x[y==0,0], x[y==0,1], 'bo', label=iris.target_names[0][:3])\n",
    "plt.plot(x[y==1,0], x[y==1,1], 'ro', label=iris.target_names[1][:3])\n",
    "plt.plot(x[y==2,0], x[y==2,1], 'yo', label=iris.target_names[2][:3])\n",
    "    \n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive quality of the model\n",
    "\n",
    "print(\"Score training = %f\" % (clf.score(x_train, y_train)))\n",
    "print(\"Score test = %f\" % (clf.score(x_test, y_test)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print()\n",
    "print(\"Confusion matrix in test:\")\n",
    "print()\n",
    "print(confusion_matrix(y_test, clf.predict(x_test))) # row: real class; column: predicted class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "\n",
    "**(1)** Do we obtain different results if the *entropy* criterion is used to split the nodes, instead of *gini* criterion?\n",
    "\n",
    "**(2)** Using *gini* criterion and *max_depth=None* (no limit in the depth) try *min_samples_split=50* and *min_samples_leaf=50* (independently). What does it happen in each case? Did you expect it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
